{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a04e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BUILDING ATTRITION PREDICTION MODEL ===\n",
      "Target: Predict the 237 employees who will leave\n",
      "Using 10 key features identified from HR analysis\n",
      "Features: ['MonthlyIncome', 'OverTime', 'YearsAtCompany', 'JobSatisfaction', 'WorkLifeBalance', 'YearsSinceLastPromotion', 'BusinessTravel', 'JobRole', 'Department', 'PerformanceRating']\n"
     ]
    }
   ],
   "source": [
    "# notebooks/3_model_training.ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../data/employee_attrition.csv')\n",
    "\n",
    "print(\"=== BUILDING ATTRITION PREDICTION MODEL ===\")\n",
    "print(f\"Target: Predict the {df['Attrition'].value_counts()['Yes']} employees who will leave\")\n",
    "\n",
    "# Convert target to binary\n",
    "df['Attrition'] = df['Attrition'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Select features based on our HR analysis\n",
    "features = ['MonthlyIncome', 'OverTime', 'YearsAtCompany', 'JobSatisfaction', \n",
    "           'WorkLifeBalance', 'YearsSinceLastPromotion', 'BusinessTravel',\n",
    "           'JobRole', 'Department', 'PerformanceRating']\n",
    "\n",
    "print(f\"Using {len(features)} key features identified from HR analysis\")\n",
    "print(\"Features:\", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57692370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We'll compare:\n",
      "1. Random Forest - For high accuracy\n",
      "2. Logistic Regression - For interpretable insights\n",
      "3. Feature Importance - To show key drivers (like we found: Income, Overtime, etc.)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Two models for comparison\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "print(\"We'll compare:\")\n",
    "print(\"1. Random Forest - For high accuracy\")\n",
    "print(\"2. Logistic Regression - For interpretable insights\")\n",
    "print(\"3. Feature Importance - To show key drivers (like we found: Income, Overtime, etc.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e57f45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed:\n",
      "X shape: (1470, 10), y shape: (1470,)\n",
      "Categorical features encoded: ['OverTime', 'BusinessTravel', 'JobRole', 'Department']\n",
      "\n",
      "Training set: 1176 samples\n",
      "Test set: 294 samples\n",
      "Attrition in training: {0: 986, 1: 190}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare features - handle categorical variables\n",
    "X = df[features].copy()\n",
    "y = df['Attrition']\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "categorical_cols = ['OverTime', 'BusinessTravel', 'JobRole', 'Department']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"Feature engineering completed:\")\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "print(f\"Categorical features encoded: {categorical_cols}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Attrition in training: {y_train.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4093f33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING MODELS ===\n",
      "\n",
      "--- Training Random Forest ---\n",
      "Accuracy: 0.837\n",
      "\n",
      "--- Training Logistic Regression ---\n",
      "Accuracy: 0.854\n",
      "\n",
      "=== MODEL COMPARISON ===\n",
      "Random Forest: 0.837 accuracy\n",
      "Logistic Regression: 0.854 accuracy\n",
      "\n",
      "ðŸŽ¯ BEST MODEL: Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "# ADD TO notebooks/3_model_training.ipynb\n",
    "\n",
    "print(\"=== TRAINING MODELS ===\")\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of attrition\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "\n",
    "print(f\"\\n=== MODEL COMPARISON ===\")\n",
    "for name, result in results.items():\n",
    "    print(f\"{name}: {result['accuracy']:.3f} accuracy\")\n",
    "\n",
    "# Show detailed performance\n",
    "best_model_name = max(results, key=lambda x: results[x]['accuracy'])\n",
    "print(f\"\\nðŸŽ¯ BEST MODEL: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13c72a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL HR RECOMMENDATIONS ===\n",
      "ðŸš¨ PRIORITY 1: ADDRESS OVERTIME\n",
      "   â€¢ Overtime increases attrition risk by 3.6x\n",
      "   â€¢ Action: Hire more staff, redistribute workload\n",
      "\n",
      "ðŸŽ¯ PRIORITY 2: IMPROVE WORK-LIFE BALANCE\n",
      "   â€¢ Every 1-point improvement in WorkLifeBalance reduces attrition risk\n",
      "   â€¢ Action: Flexible hours, remote work options\n",
      "\n",
      "ðŸ’¼ PRIORITY 3: DEPARTMENT-SPECIFIC SOLUTIONS\n",
      "   â€¢ Sales department needs immediate attention (20.6% attrition)\n",
      "   â€¢ Action: Sales-specific retention bonuses, better management\n",
      "\n",
      "ðŸ“ˆ PRIORITY 4: CAREER DEVELOPMENT\n",
      "   â€¢ Employees waiting for promotions are at higher risk\n",
      "   â€¢ Action: Clear promotion paths, regular career conversations\n",
      "\n",
      "ðŸ“Š QUANTIFIED BUSINESS IMPACT:\n",
      "   â€¢ OverTime: INCREASES risk by 3.6x\n",
      "   â€¢ Department: INCREASES risk by 1.7x\n",
      "   â€¢ WorkLifeBalance: REDUCES risk by 1.5x\n"
     ]
    }
   ],
   "source": [
    "# ADD FINAL ANALYSIS - CORRECTED VERSION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== FINAL HR RECOMMENDATIONS ===\")\n",
    "print(\"ðŸš¨ PRIORITY 1: ADDRESS OVERTIME\")\n",
    "print(f\"   â€¢ Overtime increases attrition risk by {np.exp(1.292):.1f}x\")\n",
    "print(\"   â€¢ Action: Hire more staff, redistribute workload\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ PRIORITY 2: IMPROVE WORK-LIFE BALANCE\")  \n",
    "print(\"   â€¢ Every 1-point improvement in WorkLifeBalance reduces attrition risk\")\n",
    "print(\"   â€¢ Action: Flexible hours, remote work options\")\n",
    "\n",
    "print(\"\\nðŸ’¼ PRIORITY 3: DEPARTMENT-SPECIFIC SOLUTIONS\")\n",
    "print(\"   â€¢ Sales department needs immediate attention (20.6% attrition)\")\n",
    "print(\"   â€¢ Action: Sales-specific retention bonuses, better management\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ PRIORITY 4: CAREER DEVELOPMENT\")\n",
    "print(\"   â€¢ Employees waiting for promotions are at higher risk\")\n",
    "print(\"   â€¢ Action: Clear promotion paths, regular career conversations\")\n",
    "\n",
    "# FIRST define feature_importance, THEN use it\n",
    "lr_model = results['Logistic Regression']['model']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'coefficient': lr_model.coef_[0],\n",
    "    'abs_impact': np.abs(lr_model.coef_[0])\n",
    "}).sort_values('abs_impact', ascending=False)\n",
    "\n",
    "# NOW calculate risk multipliers\n",
    "risk_multipliers = pd.DataFrame({\n",
    "    'feature': feature_importance['feature'],\n",
    "    'risk_multiplier': np.exp(feature_importance['coefficient']),\n",
    "    'impact': feature_importance['coefficient'].apply(\n",
    "        lambda x: f\"INCREASES risk by {np.exp(x):.1f}x\" if x > 0 else f\"REDUCES risk by {1/np.exp(x):.1f}x\"\n",
    "    )\n",
    "})\n",
    "\n",
    "print(f\"\\nðŸ“Š QUANTIFIED BUSINESS IMPACT:\")\n",
    "for _, row in risk_multipliers.head(3).iterrows():\n",
    "    print(f\"   â€¢ {row['feature']}: {row['impact']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7a8fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPLETE ML EVALUATION ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Detailed model evaluation\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, result \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ“Š \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m DETAILED PERFORMANCE:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# ADD TO notebooks/3_model_training.ipynb - COMPLETE ML PROCEDURES\n",
    "\n",
    "print(\"=== COMPLETE ML EVALUATION ===\")\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
    "\n",
    "# Detailed model evaluation\n",
    "for name, result in results.items():\n",
    "    print(f\"\\nðŸ“Š {name} DETAILED PERFORMANCE:\")\n",
    "    y_pred = result['predictions']\n",
    "    y_proba = result['probabilities']\n",
    "    \n",
    "    # Classification report\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Attrition', 'Attrition']))\n",
    "    \n",
    "    # ROC-AUC\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.3f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    \n",
    "    # Business metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"True Positives (Correctly predicted leavers): {tp}\")\n",
    "    print(f\"False Negatives (Missed leavers): {fn}\")\n",
    "    print(f\"False Positives (Wrongly predicted as leavers): {fp}\")\n",
    "\n",
    "# Feature Importance from Random Forest (alternative view)\n",
    "rf_model = results['Random Forest']['model']\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nðŸŒ³ RANDOM FOREST FEATURE IMPORTANCE:\")\n",
    "print(rf_importance.head(10))\n",
    "\n",
    "# PREDICTION ON NEW DATA\n",
    "print(f\"\\nðŸŽ¯ MAKING PREDICTIONS - SAMPLE EMPLOYEES AT RISK:\")\n",
    "\n",
    "# Get high-risk employees from test set\n",
    "high_risk_indices = np.where(y_proba > 0.7)[0]\n",
    "if len(high_risk_indices) > 0:\n",
    "    print(f\"Found {len(high_risk_indices)} employees with >70% attrition risk\")\n",
    "    \n",
    "    # Show some high-risk cases\n",
    "    for i in high_risk_indices[:3]:\n",
    "        emp_data = X_test.iloc[i]\n",
    "        actual_attrition = y_test.iloc[i]\n",
    "        proba = y_proba[i]\n",
    "        \n",
    "        print(f\"\\nEmployee Risk Score: {proba:.1%}\")\n",
    "        print(f\"Actual: {'LEFT' if actual_attrition == 1 else 'STAYED'}\")\n",
    "        print(f\"Features: Income=${emp_data['MonthlyIncome']}, Overtime={emp_data['OverTime']}, JobSat={emp_data['JobSatisfaction']}\")\n",
    "\n",
    "# Save prediction probabilities for later use\n",
    "test_predictions = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'probability': results['Logistic Regression']['probabilities']\n",
    "})\n",
    "test_predictions.to_csv('../results/test_predictions.csv', index=False)\n",
    "print(f\"\\nðŸ’¾ Predictions saved to 'results/test_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53b18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
